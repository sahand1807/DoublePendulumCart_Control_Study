# Double Pendulum on Cart Control Study

[![Python 3.8+](https://img.shields.io/badge/python-3.8+-blue.svg)](https://www.python.org/downloads/)
[![MuJoCo](https://img.shields.io/badge/MuJoCo-3.0+-green.svg)](https://mujoco.org/)
[![License](https://img.shields.io/badge/license-MIT-blue.svg)](LICENSE)

A comprehensive study of control strategies for stabilizing a double pendulum on a cart system at the upright equilibrium.


## ğŸ¯ Objective

Design, implement, and compare three control strategies for balancing a double-pendulum-on-cart system at the unstable upright equilibrium:

1. **LQR** (Linear Quadratic Regulator)
2. **MPC** (Model Predictive Control)
3. **PPO** (Proximal Policy Optimization)

The system is simulated using a high-fidelity **MuJoCo** physics engine through a custom **Gymnasium** environment with accurately modeled dynamics derived from Lagrangian mechanics.

---

## ğŸ”¬ System Description

### Physical Parameters

| Component | Parameter | Value |
|-----------|-----------|-------|
| **Cart** | Mass (M) | 1.0 kg |
| | Force limit | Â±20 N |
| **Link 1** | Mass (mâ‚) | 0.3 kg |
| | Length (lâ‚) | 0.5 m |
| | COM distance (lcâ‚) | 0.25 m |
| | Inertia (Iâ‚) | 0.01 kgâ‹…mÂ² |
| **Link 2** | Mass (mâ‚‚) | 0.2 kg |
| | Length (lâ‚‚) | 0.4 m |
| | COM distance (lcâ‚‚) | 0.2 m |
| | Inertia (Iâ‚‚) | 0.008 kgâ‹…mÂ² |

### State Space

**State vector**: `[x, Î¸â‚, Î¸â‚‚, áº‹, Î¸Ì‡â‚, Î¸Ì‡â‚‚]`
- `x`: Cart position (m)
- `Î¸â‚, Î¸â‚‚`: Absolute angles from vertical (rad)
  - Î¸ = 0: hanging down
  - Î¸ = Ï€: upright (target)
- `áº‹, Î¸Ì‡â‚, Î¸Ì‡â‚‚`: Velocities

**Control**: Horizontal force on cart (continuous)

**Simulation**: 100 Hz (dt = 0.01s), RK4 integrator

---

## ğŸš€ Quick Start

### Installation
```bash
# Clone repository
git clone https://github.com/sahand1807/DoublePendulumCart_Control_Study.git
cd DoublePendulumCart_Control_Study

# Create virtual environment
python -m venv venv
source venv/bin/activate 

# Install dependencies
pip install -r requirements.txt

```

---

## ğŸ¤– Controllers Implemented

### 1. LQR (Linear Quadratic Regulator)
**Status:** âœ… Complete

Classic model-based optimal control approach using linearization around upright equilibrium.

**Features:**
- Analytical solution via Continuous Algebraic Riccati Equation (CARE)
- Fast computation (~0.05 ms per step)
- Guaranteed stability within linear regime
- Region of attraction: ~32.5% of Â±30Â° test grid

**Usage:**
```bash
# Run LQR experiments with visualization
python experiments/run_lqr.py

# Real-time 3D viewer with LQR control
python experiments/run_lqr_3D.py
```

**Documentation:** See [LQR Study Report](docs/LQR_Study_Report.md)

---

### 2. PPO (Proximal Policy Optimization)
**Status:** âœ… Complete

Model-free deep reinforcement learning approach with curriculum learning.

**Features:**
- 3-level curriculum: Â±3Â° â†’ Â±6Â° â†’ Â±10Â° perturbations
- Transfer learning across difficulty levels
- Neural network policy: [64, 64] architecture
- Sin/cos angle encoding for continuous learning
- Hardware acceleration (Apple M2 GPU / CUDA support)

**Training:**
```bash
# Level 1: Train from scratch (Â±3Â° perturbations)
python experiments/train_ppo_level1.py --timesteps 500000

# Level 2: Transfer learning (Â±6Â° perturbations)
python experiments/train_ppo_level2.py \
    --transfer-from results/ppo_level1/best_model/best_model.zip \
    --timesteps 200000

# Level 3: Transfer learning (Â±10Â° perturbations)
python experiments/train_ppo_level3.py \
    --transfer-from results/ppo_level2/best_model/best_model.zip \
    --timesteps 500000
```

**Evaluation & Visualization:**
```bash
# Evaluate trained models
python experiments/evaluate_ppo_level1.py
python experiments/evaluate_ppo_level2.py
python experiments/evaluate_ppo_level3.py

# Generate animations
python experiments/render_ppo_level1.py
python experiments/render_ppo_level2.py
python experiments/render_ppo_level3.py

# Plot learning curves
python utils/plot_learning_curves.py --log-dir results/ppo_level1
```

**Performance:**
- Level 1: 100% success rate, 1.2s settling time
- Level 2: 100% success rate, 2.3s settling time
- Level 3: 95% success rate, 3.6s settling time
- Total training time: ~108 minutes (M2 GPU)

**Documentation:** See [PPO Study Report](docs/PPO_Study_Report.md)

---

### 3. MPC (Model Predictive Control)
**Status:** ğŸš§ Planned

Nonlinear optimal control with constraint handling via receding horizon optimization.

---

## ğŸ“Š Results Summary

| Controller | Success Rate | Settling Time | Cart Drift | Region of Attraction |
|------------|--------------|---------------|------------|---------------------|
| **LQR** | 100% (within ROA) | 0.41s | 0.15m | ~32.5% (Â±30Â° grid) |
| **PPO Level 1** | 100% | 1.2s | 0.12m | Â±3Â° (trained) |
| **PPO Level 2** | 100% | 2.3s | 0.24m | Â±6Â° (trained) |
| **PPO Level 3** | 95% | 3.6s | 0.42m | Â±10Â° (trained) |
| **MPC** | TBD | TBD | TBD | TBD |

**Key Findings:**
- **LQR:** Fastest response but limited to small perturbations
- **PPO:** Wider capability range, handles 2Ã— larger perturbations than LQR
- **Hybrid Approach Recommended:** Use PPO for large deviations, switch to LQR for fast settling

---

## ğŸ“ Project Structure

```
DoublePendulumCart_Control_Study/
â”œâ”€â”€ env/                          # Gymnasium environment
â”‚   â”œâ”€â”€ double_pendulum_cart_env.py
â”‚   â”œâ”€â”€ angle_wrapper.py          # Sin/cos encoding + curriculum
â”‚   â””â”€â”€ double_pendulum_cart.xml  # MuJoCo model
â”œâ”€â”€ controllers/                  # Controller implementations
â”‚   â”œâ”€â”€ lqr_controller.py
â”‚   â”œâ”€â”€ ppo_controller.py
â”‚   â””â”€â”€ base_controller.py
â”œâ”€â”€ experiments/                  # Training & evaluation scripts
â”‚   â”œâ”€â”€ train_ppo_level*.py
â”‚   â”œâ”€â”€ evaluate_ppo_level*.py
â”‚   â”œâ”€â”€ render_ppo_level*.py
â”‚   â”œâ”€â”€ run_lqr.py
â”‚   â””â”€â”€ run_lqr_3D.py
â”œâ”€â”€ utils/                        # Visualization utilities
â”‚   â””â”€â”€ plot_learning_curves.py
â”œâ”€â”€ docs/                         # Documentation & reports
â”‚   â”œâ”€â”€ PPO_Study_Report.md
â”‚   â”œâ”€â”€ LQR_Study_Report.md
â”‚   â””â”€â”€ theory.md
â”œâ”€â”€ results/                      # Saved models & plots
â”‚   â”œâ”€â”€ ppo_level1/
â”‚   â”œâ”€â”€ ppo_level2/
â”‚   â””â”€â”€ ppo_level3/
â”œâ”€â”€ assets/                       # Figures for documentation
â”‚   â”œâ”€â”€ ppo/
â”‚   â””â”€â”€ lqr/
â””â”€â”€ requirements.txt
```

---

## ğŸ”§ Testing

```bash
# Test environment step function
python tests/test_env_step.py

# Test visualization
python test_visualization.py
```